{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Vision**\n",
    "\n",
    "This project implements a set of neural networks that identify a car's make and model from an image of the car. It draws on the Stanford cars dataset, which contains 16,000 images of 196 different types of cars, and it implements four different convolutional neural network structures. By implementing multiple different types of networks, the results of the networks could be compared.  The base code for the networks was drawn from the git repo located at [1].\n",
    " \n",
    "The most likely application of a neural network like this would be in law enforcement.  In this role, it would likely be used to identify vehicles used in crimes and caught on camera.  Alternatively It could also be used as part of a stolen vehicle database to identify stolen vehicles. Also, it could be used to check for a certain type of vehicle in video if looking for a specific vehicle.  Outside of law enforcement, It could also be used as security to prevent vehicles of unknown type from entering secure areas.  People could use it to unlock their house or issue an alert if a specific type of car drives up, in case one wants to know when one’s spouse returns.  Finally, it could be used as an example project for selling students on computer science and neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background and Implementation**\n",
    "\n",
    "This project implements four convolutional neural networks (CNNs).  Descriptions of a CNN and how they work can be found in Chapter 5 of Francois Chollet’s book *Deep Learning with Python* or at the GitHub repo located at [2].  Three networks are model networks from Keras, specifically the VGG16, VGG19, and Inception V3 model networks, and the fourth is a custom network that I wrote.  Documentation on the VGG16 and VGG19 models used can be found at [3], [4], and [5], and documentation on the Inception V3 network can be found at [6].   My network and both VGG16 and VGG19 are sequential CNNs.  These networks have only one possible path from one layer to the next.  Unlike my network, however, both VGG16 and VGG19 use multiple convolutional layers between each max pooling layer [5], as shown in the diagram in Figure 1, drawn from [3].\n",
    "\n",
    "<img src=Pictures/vgg16.png>\n",
    "\n",
    "\n",
    "<center> Figure 1: Layout of VGG16 Network Structure Showing Multiple Convolution Layers Between Pooling Layers </center>\n",
    "\n",
    "This allows the use of smaller filters since overlapping smaller filters are combined to make one larger filter.  For instance, to make a 5x5 filter, five 3x3 filters are used, one centered on the same point as the 5x5 filter, and one centered on each of the central 3x3 filter’s corners  This is shown in diagram form in Figure 2, taken from [5].\n",
    "\n",
    "<img src=Pictures/5x5from3x3.png>\n",
    "\n",
    "<center> Figure 2: Diagram Showing How To Derive a 5x5 Convolution From Two Layers of 3x3 Convolutions </center>\n",
    "\n",
    "This, in turn, reduces the number of parameters which the model needs to store [5].  In the 5x5 filter case, 25 (5 * 5) parameters are needed, one for each pixel in the filter array.  In the 3x3 filter case, each filter array requires 9 (3 * 3) parameters, but, since the 3x3 filter case requires two layers, it also needs two filter arrays, so the total number of parameters needed in the 3x3 case is 18.  This concept, however, can be expanded past creating 5x5 filters from 3x3 filters.  With more layers, larger filters can be made.  A 7x7 filter can be made with three 3x3 convolutional layers and an 11x11 filter can be made from four 3x3 convolutional layers.  The parameter savings get larger as the filters do as well.  Replacing a 7x7 filter layer with three 3x3 filter layers saves 22 parameters per node (7 * 7 - (3 * 3) * 3) and replacing a 11x11 filter layer with four 3x3 filter layers saves 76 parameters per node (11* 11 - (3 * 3) * 4).  For the VGG16 network, these changes save around 30,000 parameters.  Beyond this useful attribute, VGG16 and VGG19 have few distinguishing features.\n",
    "\n",
    "My network, while similar in basic structure to VGG16 and VGG19, is much smaller.  It has only three convolutional layers and three max pooling layers.  The convolutional layers have 64, 128, and 256 nodes, respectively, and use relu activation functions.  The max pooling layers use 2x2 filters.  These layers are followed by a 256-node dense layer with a relu activation function, and the output comes from a 20-node dense layer with a softmax activation function.\n",
    "\n",
    "While VGG16 and VGG19 use relatively simply and straightforward architectures, Inception V3, originally known as GoogLeNet, uses a much more tangled architecture.  In the original Inception network, when entering a layer, the signal goes four ways.  It feeds into a 1x1 convolution layer, a 1x1 convolution layer followed by a 3x3 convolution layer, a 1x1 convolution layer followed by a 5x5 convolution layer, and a 3x3 max pooling layer followed by a 1x1 convolution layer.  The 1x1 convolution layers decrease the number of inputs significantly, reducing the computational costs of running 1x1, 3x3, and 5x5 convolution filters for every layer.  The outputs of each of these paths are concatenated and the result is fed into the next layer.  A diagram of this inter-layer network drawn from [7] can be found in Figure 3. \n",
    "\n",
    "<img src=Pictures/imagenet_inception_module.png>\n",
    "\n",
    "<center> Figure 3: Theoretical Layout of One Layer of the Inception V3 Network </center>\n",
    "\n",
    "By using multiple different filter sizes, the network can identify features of different sizes, meaning that it is not reliant on receiving images of a certain scale.  Using multiple filter sizes also reduces overfitting as the Inception network does not need to be as deep as a similar sequential networks to achieve the same performance, and depth directly correlates to the likelihood of overfitting [6].  Inception V3 uses the same basic model as laid out above, but it makes a number of changes.  Like VGG16 and VGG19, Inception V3 replaces 5x5 filter layers with two 3x3 filter layers.  However, Inception V3 goes further.  It replaces the 3x3 filters layers with another pair of layers, one that uses a 3x1 filter and a second that uses a 1x3 filter.  The combination of these two layers replicates the effects of using a 3x3 filter while decreasing the computation costs by a third [6].  The resulting structure is shown in Figure 4 [6].\n",
    "\n",
    "<img src=Pictures/incepV3FinalStructure.png>\n",
    "\n",
    "<center> Figure 4: Practical Layout of One Layer of the Inception V3 Network (Here, n=3) </center>\n",
    "\n",
    "Finally, Inception V3 implements label smoothing.  When a network uses label smoothing, it assumes that output values close to 1 or 0 are 1 or 0 [8].  This prevents the model from looking to push the final few steps to surety, an action which causes overfitting.  Inception V3 is by far the most complex of the networks implemented for this project.\n",
    "\n",
    "Convolutional neural networks were chosen for the task of identifying cars in pictures because they excel when looking for specific features repeated from sample to sample.  This is especially relevant for an image content identification algorithm like this one.  To identify a specific car, the neural network needs to identify a specific set of shapes unique to each car, and then look for those shapes in a picture.  If two cars possess a similar shape e.g. the circular headlight of a Jeep Wrangler and the circular taillight of a Ford GT, the other shapes are used to identify the vehicle.  To allow for sufficient differentiation of identification sets, the neural network needs to be large enough to accomodate the number of features needed.  However, if the network gets too large, it starts to “memorize” the training images, especially if it is trained repeatedly on the same data.  This is the phenomenon of overfitting, and it requires that CNNs hit a sweet spot when it comes to size.  I chose VGG16, VGG19, and Inception V3 because they allowed me to contrast the effects of using a sequential network (VGG) versus a multi-feature extractor (Inception) and the effects of adding depth (VGG16 versus VGG19).  I added a network of my own so that I had more control over the various parts of the network.\n",
    "\n",
    "While convolutional neural networks work very well for image content identification, other types of networks do not work as well.  A recursive neural network (RNN) would not work as well, since they work best when taking data in order, such as text.  An example of a good use of a RNN would be a movie review sentiment analyzer, because it could take the words of the review one at a time, adjust the outputs based on the previously learned sentiment, and then take the next word.  A dense network would also not work well.  Although they can sometimes work if enough brute force is put behind them, they struggle to identify features nearly as well as CNNs, and thus would be a poor choice for an application like this.  Further information on these network structures can be found at [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The best network of the four networks implemented was the VGG16 network.  After ten epochs of training and then another ten epochs of retraining, it returned an accuracy of 55.84%.  This fell short of the results which the owner of the repo on which the project was based reported [1].  He reported that he got 66.11% accuracy with VGG16.  There are two likely reasons for this shortfall.  First, the repo owner put the network through 500 epochs of training and 500 epochs of retraining while I only did ten of each.  Second, I used significantly fewer training images.  To allow me to build a usable network on my own, I cut the number of training images by a factor of eight and the testing set by a factor of 26.  This likely led the network to overfit to the testing data since it is made to handle a much larger data set.  VGG19 trailed its smaller sibling significantly, returning an accuracy of 36.26%.\n",
    "\n",
    "The Inception V3 network did a bit better than VGG19, but fell well short of VGG16.  It produced an accuracy of 38.36%.  This correlated to the experience of the repo owner, who found that neither Inception V3 nor VGG19 produced as high an accuracy as VGG16 [1].  This happened because the depth of Inception V3, and, to a lesser extent, VGG19, works against them.  Inception V3, VGG16, and VGG19 were all designed to use the ImageNet dataset, which has 1,000 categories and about 1.2 million training images.  This is more than three orders of magnitude more images that were used in this project, which had 1,043 training images.  Because Inception V3 is a much larger network than VGG16, as is, to a lesser extent, VGG19, they will overfit to the smaller dataset more quickly, which will lower the accuracy. All three networks showed signs of overfitting by the end of the fourth epoch, so this dataset was definitely too small.\n",
    "\n",
    "While my network did not do as well as the others, it eventually produced meaningful results.  I managed to get it up to 24.43% accuracy.  Most iterations of my network suffered from the vanishing gradient problem.  Consequently, by the time signal reached the bottom, it was basically zero, so the network was basically guessing.  To fix this, I reduced the number of convolutional layers from 5 or 7 in previous runs to 3.  However, this network still shows signs of overfitting around the sixth epoch of training, so it may still be too large.  Although 24.43% cannot be considered excellent, it is significantly better than what amounted to random guesswork.\n",
    "\n",
    "The best network that I found returned an accuracy of 88.7% [9].  It used the whole dataset, not the cut-down version that I used, and worked around the ResNet-152 model architecture, a model that is newer and better than Inception V3.\n",
    "\n",
    "One example graph showing the accuracy for each epoch of training, both initial and final, of each model can be found in the Graphs folder of this repo, or in showcase.ipynb.\n",
    "\n",
    "One thing that I would like to do to continue working with the project is to do it again with the complete Stanford Cars dataset, preferably with additional photos.  The cut-down dataset I used for the project helped get it off the ground, but it led to significant overfitting, and I would like to see how far I can push these architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implications**\n",
    "\n",
    "This project has major potential privacy implications.  While it may lead to more criminals who get caught after their car gets tracked, it would decrease the privacy of ordinary citizens.  It would potentially help governments track people’s movements, especially if applied to speed cameras and other devices which police can access without a warrant.  Similarly, private firms could build a tool like this with which they can track people’s movements, and then monetize the data.  It could also lead to ads targeted based on movements.  Although there would be a considerable number of false positives, the additional information would probably have some value.  Perhaps worst of all, an abusive husband could use this system to monitor his wife's comings and goings, allowing him additional control over her life.  To me, this would be utterly abhorrent.  In my opinion, the widespread implementation of this project in public life would decrease the quality of life overall for the public at large because of the potential costs to personal privacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliography**\n",
    "\n",
    "[1] https://github.com/michalgdak/car-recognition\n",
    "\n",
    "[2] https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "[3] https://neurohive.io/en/popular-networks/vgg16/\n",
    "\n",
    "[4] https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\n",
    "\n",
    "[5] https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11\n",
    "\n",
    "[6] https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n",
    "\n",
    "[7] https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n",
    "\n",
    "[8] http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_3.pdf\n",
    "\n",
    "[9] https://github.com/foamliu/Car-Recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
