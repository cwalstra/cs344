{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook contains the code for the final project of CS 344 Artificial Intelligence at Calvin College.\n",
    "\n",
    "It creates a number of different networks, all aimed at using the Stanford Cars dataset to identify car brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 22 sty 2018\n",
    "3D Object Representations for Fine-Grained Categorization\n",
    "Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei\n",
    "4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013.\n",
    "@author: mgdak\n",
    "'''\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import shutil as sh\n",
    "from pprint import pprint\n",
    "import scipy.io as sio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "import shutil as sh\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras import callbacks\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "train_data_dir = 'cars_train'\n",
    "val_data_dir = 'cars_test'\n",
    "nb_train_samples = 843\n",
    "nb_val_samples = 831"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Global Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(learning_rate, lr_decay):\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        \"/home/cjw44/allCars/car_ims/cars_test\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(\"./carRecognition_finalModel\" + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"./carRecognition_finalModel\" + '.h5')\n",
    "    print('Loaded model from disk')\n",
    "\n",
    "    # evaluate loaded model on test data\n",
    "    sgd = SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "    loaded_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    score = loaded_model.evaluate_generator(test_generator, 3957 / 16, workers=6)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1] * 100))\n",
    "\n",
    "# Creates an array with following values: picture name, picture category ID, train/validation label       \n",
    "def readData(matFile):\n",
    "    content = sio.loadmat(matFile)\n",
    "    data = [(_[0][0][:], _[5][0][0], _[6][0][0]) for _ in content['annotations'][0]]\n",
    "    return data\n",
    "\n",
    "\n",
    "# Creates an array of all classes\n",
    "def readClasses(matFile):\n",
    "    content = sio.loadmat(matFile)\n",
    "    classes = [(_[0]) for _ in content['class_names'][0]]\n",
    "    return classes\n",
    "\n",
    "\n",
    "# Movces raw data (pictures) into respective category subfolders with train/validation division \n",
    "def dataPreprocessing(dataDir, labelsFile):\n",
    "    data = readData(labelsFile)\n",
    "    classes = readClasses(labelsFile)\n",
    "    print(\"---------------\")\n",
    "    for recData in data:\n",
    "        if recData[2] == 1:\n",
    "            # validation set\n",
    "            os.makedirs(dataDir + \"/\" + val_data_dir + \"/\" + classes[recData[1] - 1] + \"/\", exist_ok=True)\n",
    "            sh.move(dataDir + \"/\" + recData[0][8:],\n",
    "                    dataDir + \"/\" + val_data_dir + \"/\" + classes[recData[1] - 1] + \"/\" + recData[0][8:])\n",
    "        else:\n",
    "            os.makedirs(dataDir + \"/\" + train_data_dir + \"/\" + classes[recData[1] - 1] + \"/\", exist_ok=True)\n",
    "            sh.move(dataDir + \"/\" + recData[0][8:],\n",
    "                    dataDir + \"/\" + train_data_dir + \"/\" + classes[recData[1] - 1] + \"/\" + recData[0][8:])  # train set\n",
    "\n",
    "\n",
    "# serializes the trained model and its weights\n",
    "def serializeModel(model, fileName):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(fileName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(fileName + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "def prepareDataGenerators(batchSize, srcImagesDir, labelsFile):\n",
    "    classes = readClasses(labelsFile)\n",
    "    # this is the augmentation configuration used for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    # this is the augmentation configuration used for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'car_ims_dir/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        srcImagesDir + \"/\" + train_data_dir + \"/\",  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 299x299\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical')  # since we use categorical_crossentropy loss, we need categorical labels\n",
    "    # this is a similar generator, for validation data\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        srcImagesDir + \"/\" + val_data_dir + \"/\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical')\n",
    "    return classes, train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions that allow for training and using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVGG16Architecture(classes, dropoutRate):\n",
    "    # create the base pre-trained model\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in enumerate(base_model.layers):\n",
    "        layer[1].trainable = False\n",
    "\n",
    "    # flatten the results from conv block\n",
    "    x = Flatten()(base_model.output)\n",
    "\n",
    "    # add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    # add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    # add logistic layer with all car classes\n",
    "    predictions = Dense(20, activation='softmax', kernel_initializer='random_uniform',\n",
    "                        bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions')(\n",
    "        x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def getVGG19Architecture(classes, dropoutRate):\n",
    "    # create the base pre-trained model\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in enumerate(base_model.layers):\n",
    "        layer[1].trainable = False\n",
    "\n",
    "    # flatten the results from conv block\n",
    "    x = Flatten()(base_model.output)\n",
    "\n",
    "    # add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    # add another fully connected layers with batch norm and dropout\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    # add logistic layer with all car classes\n",
    "    predictions = Dense(20, activation='softmax', kernel_initializer='random_uniform',\n",
    "                        bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions')(\n",
    "        x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def getInceptionV3Architecture(classes, dropoutRate):\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # InceptionV3\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu', kernel_initializer='random_uniform', bias_initializer='random_uniform',\n",
    "              bias_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    # add Dropout regularizer\n",
    "    x = Dropout(dropoutRate)(x)\n",
    "\n",
    "    # and a logistic layer with all car classes\n",
    "    predictions = Dense(len(classes), activation='softmax', kernel_initializer='random_uniform',\n",
    "                        bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions')(\n",
    "        x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in enumerate(base_model.layers):\n",
    "        layer[1].trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def getMyCNN(classes, dropoutRate):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(256, 4, activation='relu', input_shape=(224, 224, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(256, 4, activation='relu'))\n",
    "    model.add(Conv2D(512, 4, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(512, 4, activation='relu'))\n",
    "    model.add(Conv2D(512, 4, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='random_uniform', bias_initializer='random_uniform',\n",
    "                    bias_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "    model.add(Dense(20, activation='softmax', kernel_initializer='random_uniform',\n",
    "                        bias_initializer='random_uniform', bias_regularizer=regularizers.l2(0.01), name='predictions'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that define the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setLayersToRetrain(model, modelArchitecture):\n",
    "    if modelArchitecture == 'InceptionV3':\n",
    "        # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "        # the first 249 layers and unfreeze the rest:\n",
    "        for layer in model.layers[:249]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        for layer in model.layers[249:]:\n",
    "            layer.trainable = True\n",
    "    elif modelArchitecture == 'VGG16':\n",
    "        # train the last conv block\n",
    "        for layer in model.layers[:15]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        for layer in model.layers[15:]:\n",
    "            layer.trainable = True\n",
    "    elif modelArchitecture == 'VGG19':\n",
    "        # train the last conv block\n",
    "        for layer in model.layers[:17]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        for layer in model.layers[17:]:\n",
    "            layer.trainable = True\n",
    "    else:\n",
    "        for layer in model.layers[:5]:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[5:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "def initialTraining(optimizerLastLayer, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator,\n",
    "                    model, modelArchitecture, lr_decay, learningRate):\n",
    "    # compile the model and train the top layer only\n",
    "\n",
    "    rms = RMSprop(decay=lr_decay, lr=learningRate)\n",
    "    model.compile(optimizer=rms, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    earlystop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode='auto')\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batchSize,\n",
    "        epochs=noOfEpochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_val_samples // batchSize,\n",
    "        callbacks=[earlystop])\n",
    "    plt.plot(history.history['val_acc'], 'r')\n",
    "    plt.plot(history.history['acc'], 'b')\n",
    "    plt.title('Performance of model ' + modelArchitecture)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs No')\n",
    "    plt.savefig(savedModelName + '_initialModel_plot.png')\n",
    "    serializeModel(model, savedModelName + \"_initialModel\")\n",
    "\n",
    "\n",
    "def finetuningTraining(learningRate, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator,\n",
    "                       model, lr_decay):\n",
    "    # we need to recompile the model for these modifications to take effect\n",
    "    # we use SGD with a low learning rate\n",
    "    sgd = SGD(lr=learningRate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    earlystop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode='auto')\n",
    "\n",
    "    # we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "    # alongside the top Dense layers\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batchSize,\n",
    "        epochs=noOfEpochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_val_samples // batchSize,        callbacks=[earlystop])\n",
    "    plt.clf()\n",
    "    plt.plot(history.history['val_acc'], 'r')\n",
    "    plt.plot(history.history['acc'], 'b')\n",
    "    plt.savefig(savedModelName + '_finalModel_plot.png')\n",
    "    serializeModel(model, savedModelName + \"_finalModel\")\n",
    "\n",
    "\n",
    "def model(learningRate, optimizerLastLayer, noOfEpochs, batchSize, savedModelName, srcImagesDir, labelsFile,\n",
    "          modelArchitecture, dropoutRate, lr_decay):\n",
    "    classes, train_generator, validation_generator = prepareDataGenerators(batchSize, srcImagesDir, labelsFile)\n",
    "\n",
    "    if modelArchitecture == 'VGG16':\n",
    "        model = getVGG16Architecture(classes, dropoutRate)\n",
    "    elif modelArchitecture == 'VGG19':\n",
    "        model = getVGG19Architecture(classes, dropoutRate)\n",
    "    elif modelArchitecture == \"InceptionV3\":\n",
    "        model = getInceptionV3Architecture(classes, dropoutRate)\n",
    "    else:\n",
    "        model = getMyCNN(classes, dropoutRate)\n",
    "\n",
    "    initialTraining(optimizerLastLayer, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator,\n",
    "                    model, modelArchitecture, lr_decay, learningRate)\n",
    "\n",
    "    setLayersToRetrain(model, modelArchitecture)\n",
    "\n",
    "\n",
    "    finetuningTraining(learningRate, noOfEpochs, batchSize, savedModelName, train_generator, validation_generator,\n",
    "                       model, lr_decay)\n",
    "\n",
    "    evaluate(learningRate, lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    pprint(args)\n",
    "    if args.process_data:\n",
    "        dataPreprocessing(args.car_ims_dir, args.car_ims_labels)\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    model(args.learning_rate,\n",
    "          args.optimizer_last_layer,\n",
    "          args.no_of_epochs,\n",
    "          args.batch_size,\n",
    "          args.saved_model_name,\n",
    "          args.car_ims_dir,\n",
    "          args.car_ims_labels,\n",
    "          args.model,\n",
    "          args.dropout_rate,\n",
    "          args.lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for starting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args[\"process_data\"] = False\n",
    "args[\"car_ims_dir\"] = \"/home/cjw44/allCars/car_ims\"\n",
    "args[\"car_ims_labels\"] = \"/home/cjw44/CarData/cars_annos\"\n",
    "args[\"learning_rate\"] = 0.001\n",
    "args[\"dropout_rate\"] = 0.7\n",
    "args[\"lr_decay\"] = 1e-4\n",
    "args[\"optimizer_last_layer\"] = \"RMSPROP\"  # options: ['ADAGRAD', 'ADADELTA', 'ADAM', 'RMSPROP', 'MOM']\n",
    "args[\"model\"] = \"MyCNN\" # options: ['VGG19', 'VGG16', 'InceptionV3', \"MyCNN\"]\n",
    "args[\"no_of_epochs\"] = 3\n",
    "args[\"batch_size\"] = 64\n",
    "args[\"saved_model_name\"] = \"carRecognition\"\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
